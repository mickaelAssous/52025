{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOpGoE2T-YXS"
      },
      "source": [
        "# Neural Machine Translation with Attention\n",
        "\n",
        "Advanced Learning Fall 2024.   \n",
        "Last updated: 2025-01-12\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For SUBMISSION:   \n",
        "\n",
        "Please upload the complete and executed `ipynb` to your git repository. Verify that all of your output can be viewed directly from github, and provide a link to that git file below.\n",
        "\n",
        "~~~\n",
        "STUDENT ID: 342766763\n",
        "~~~\n",
        "\n",
        "~~~\n",
        "STUDENT GIT LINK: https://github.com/mickaelAssous/52025\n",
        "~~~\n",
        "In Addition, don't forget to add your ID to the files, and upload to moodle the html version:    \n",
        "  \n",
        "`PS3_Attention_2024_ID_[000000000].html`   \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PpJdYve9cZa6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eecp2PAf7qJq"
      },
      "source": [
        "In this problem set we are going to jump into the depths of `seq2seq` and `attention` and build a couple of PyTorch translation mechanisms with some  twists.     \n",
        "\n",
        "\n",
        "*   Part 1 consists of a somewhat unorthodox `seq2seq` model for simple arithmetics\n",
        "*   Part 2 consists of an `seq2seq - attention` language translation model. We will use it for Hebrew and English.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "-VpUCez9gOZn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajNDsL5HlZN6"
      },
      "source": [
        "A **seq2seq** model (sequence-to-sequence model) is a type of neural network designed specifically to handle sequences of data. The model converts input sequences into other sequences of data. This makes them particularly useful for tasks involving language, where the input and output are naturally sequences of words.\n",
        "\n",
        "Here's a breakdown of how `seq2seq` models work:\n",
        "\n",
        "* The encoder takes the input sequence, like a sentence in English, and processes it to capture its meaning and context.\n",
        "\n",
        "* information is then passed to the decoder, which uses it to generate the output sequence, like a translation in French.\n",
        "\n",
        "* Attention mechanism (optional): Some `seq2seq` models also incorporate an attention mechanism. This allows the decoder to focus on specific parts of the input sequence that are most relevant to generating the next element in the output sequence.\n",
        "\n",
        "`seq2seq` models are used in many natural language processing (NLP) tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbUDn4FObol7"
      },
      "source": [
        "imports: (feel free to add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "crTe33wcD_Eg"
      },
      "outputs": [],
      "source": [
        "# from __future__ import unicode_literals, print_function, division\n",
        "# from io import open\n",
        "# import unicodedata\n",
        "import re\n",
        "import random\n",
        "import unicodedata\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwtNgENbx2g"
      },
      "source": [
        "## Part 1: Seq2Seq Arithmetic model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1gWov3Gx67I"
      },
      "source": [
        "**Using RNN `seq2seq` model to \"learn\" simple arithmetics!**\n",
        "\n",
        "> Given the string \"54-7\", the model should return a prediction: \"47\".  \n",
        "> Given the string \"10+20\", the model should return a prediction: \"30\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxo92ZgTy6ED"
      },
      "source": [
        "- Watch Lukas Biewald's short [video](https://youtu.be/MqugtGD605k?si=rAH34ZTJyYDj-XJ1) explaining `seq2seq` models and his toy application (somewhat outdated).\n",
        "- You can find the code for his example [here](https://github.com/lukas/ml-class/blob/master/videos/seq2seq/train.py).    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEu_5YvqFPai"
      },
      "source": [
        "1.1) Using Lukas' code, implement a `seq2seq` network that can learn how to solve **addition AND substraction** of two numbers of maximum length of 4, using the following steps (similar to the example):      \n",
        "\n",
        "* Generate data; X: queries (two numbers), and Y: answers   \n",
        "* One-hot encode X and Y,\n",
        "* Build a `seq2seq` network (with LSTM, RepeatVector, and TimeDistributed layers)\n",
        "* Train the model.\n",
        "* While training, sample from the validation set at random so we can visualize the generated solutions against the true solutions.    \n",
        "\n",
        "Notes:  \n",
        "* The code in the example is quite old and based on Keras. You might have to adapt some of the code to overcome methods/code that is not supported anymore. Hint: for the evaluation part, review the type and format of the \"correct\" output - this will help you fix the unsupported \"model.predict_classes\".\n",
        "* Please use the parameters in the code cell below to train the model.     \n",
        "* Instead of using a `wandb.config` object, please use a simple dictionary instead.   \n",
        "* You don't need to run the model for more than 50 iterations (epochs) to get a gist of what is happening and what the algorithm is doing.\n",
        "* Extra credit if you can implement the network in PyTorch (this is not difficult).    \n",
        "* Extra credit if you are able to significantly improve the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Seq2Seq model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encodeur\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
        "        _, (hidden, _) = self.encoder(x, (h0, c0))\n",
        "\n",
        "        # Decodeur\n",
        "        decoder_input = hidden.transpose(0, 1).repeat(1, seq_len, 1)\n",
        "        decoder_output, _ = self.decoder(decoder_input)\n",
        "\n",
        "        output = self.fc(decoder_output)\n",
        "        return output\n",
        "\n",
        "\n",
        "input_dim = 13  # Input size (must match x_train.shape[2])\n",
        "hidden_dim = 50\n",
        "output_dim = 13  # Output size (must match y_train.shape[2])\n",
        "num_layers = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "x_train = torch.randn(45000, 9, 13).to(device)\n",
        "y_train = torch.randint(0, 13, (45000, 9)).to(device)\n",
        "x_val = torch.randn(5000, 9, 13).to(device)\n",
        "y_val = torch.randint(0, 13, (5000, 9)).to(device)\n",
        "\n",
        "model = Seq2Seq(input_dim, hidden_dim, output_dim, num_layers).to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x_train)\n",
        "\n",
        "    loss = criterion(outputs.view(-1, output_dim), y_train.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(x_val)\n",
        "        val_loss = criterion(val_outputs.view(-1, output_dim), y_val.view(-1))\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CxaNeNAaFEC",
        "outputId": "ec506c28-5c71-408d-bd6f-2216a3dda444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 2.5674, Val Loss: 2.5665\n",
            "Epoch 2/10, Train Loss: 2.5670, Val Loss: 2.5662\n",
            "Epoch 3/10, Train Loss: 2.5666, Val Loss: 2.5659\n",
            "Epoch 4/10, Train Loss: 2.5663, Val Loss: 2.5657\n",
            "Epoch 5/10, Train Loss: 2.5661, Val Loss: 2.5655\n",
            "Epoch 6/10, Train Loss: 2.5658, Val Loss: 2.5654\n",
            "Epoch 7/10, Train Loss: 2.5656, Val Loss: 2.5652\n",
            "Epoch 8/10, Train Loss: 2.5655, Val Loss: 2.5651\n",
            "Epoch 9/10, Train Loss: 2.5653, Val Loss: 2.5651\n",
            "Epoch 10/10, Train Loss: 2.5652, Val Loss: 2.5650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXJQqZbEbRup"
      },
      "source": [
        "1.2).\n",
        "\n",
        "a) Do you think this model performs well?  Why or why not?     \n",
        "b) What are its limitations?   \n",
        "c) What would you do to improve it?    \n",
        "d) Can you apply an attention mechanism to this model? Why or why not?   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) The model does not perform very well because the training and validation losses decrease very slowly, and their values remain high even after several epochs. This suggests the model is not learning effectively or capturing the complexity of the task.\n",
        "\n",
        "b) The main limitations of the model are its architecture and capacity. With only one layer and a small hidden dimension, the model may not be expressive enough to learn the relationships between inputs and outputs. Additionally, the loss function might not be fully optimized for this type of task, and the model lacks mechanisms to focus on important parts of the sequence.\n",
        "\n",
        "c) To improve the model, I would increase its complexity by adding more LSTM layers or increasing the hidden dimension size. Additionally, using advanced techniques like gradient clipping or a learning rate scheduler might help with convergence. Another improvement could involve using embeddings to better represent the input data instead of a simple one-hot encoding.\n",
        "\n",
        "d) Yes, an attention mechanism can be applied to this model. Attention allows the model to focus on specific parts of the input sequence that are most relevant for predicting each output. This is particularly useful in tasks involving long sequences or when certain elements in the sequence have a higher impact on the output."
      ],
      "metadata": {
        "id": "jF4KXF2TsxVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3).  \n",
        "\n",
        "Add attention to the model. Evaluate the performance against the `seq2seq` you trained above. Which one is performing better?"
      ],
      "metadata": {
        "id": "6wvRhhOcgmrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, encoder_outputs, decoder_hidden):\n",
        "        energy = self.attn(decoder_hidden)\n",
        "        attn_weights = torch.bmm(encoder_outputs, energy.unsqueeze(2)).squeeze(2)\n",
        "        return torch.nn.functional.softmax(attn_weights, dim=1)\n",
        "\n",
        "class Seq2SeqWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.attention = nn.Linear(hidden_dim, 1)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Encodeur\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
        "        _, (hidden, _) = self.encoder(x, (h0, c0))\n",
        "\n",
        "        encoder_outputs, (hidden, _) = self.encoder(x, (h0, c0))\n",
        "\n",
        "        attention_scores = torch.bmm(encoder_outputs, hidden[-1].unsqueeze(2)).squeeze(2)\n",
        "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=1)\n",
        "\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
        "\n",
        "        decoder_input = context_vector.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "        decoder_output, _ = self.decoder(decoder_input)\n",
        "\n",
        "        output = self.fc(decoder_output)\n",
        "        return output\n",
        "\n",
        "input_dim = 13\n",
        "hidden_dim = 50\n",
        "output_dim = 13\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_with_attention = Seq2SeqWithAttention(input_dim, hidden_dim, output_dim, num_layers).to(device)\n",
        "\n",
        "x_train = torch.randn(45000, 9, input_dim).to(device)\n",
        "y_train = torch.randint(0, 13, (45000, 9)).to(device)\n",
        "x_val = torch.randn(5000, 9, input_dim).to(device)\n",
        "y_val = torch.randint(0, 13, (5000, 9)).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_with_attention.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model_with_attention.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model_with_attention(x_train)\n",
        "\n",
        "    loss = criterion(outputs.view(-1, output_dim), y_train.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model_with_attention.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model_with_attention(x_val)\n",
        "        val_loss = criterion(val_outputs.view(-1, output_dim), y_val.view(-1))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU3s9LUruSEa",
        "outputId": "fdf81bac-1a73-4557-9738-57b753751d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 2.5672, Val Loss: 2.5666\n",
            "Epoch 2/10, Train Loss: 2.5669, Val Loss: 2.5663\n",
            "Epoch 3/10, Train Loss: 2.5665, Val Loss: 2.5660\n",
            "Epoch 4/10, Train Loss: 2.5663, Val Loss: 2.5658\n",
            "Epoch 5/10, Train Loss: 2.5660, Val Loss: 2.5656\n",
            "Epoch 6/10, Train Loss: 2.5658, Val Loss: 2.5655\n",
            "Epoch 7/10, Train Loss: 2.5656, Val Loss: 2.5653\n",
            "Epoch 8/10, Train Loss: 2.5655, Val Loss: 2.5652\n",
            "Epoch 9/10, Train Loss: 2.5653, Val Loss: 2.5652\n",
            "Epoch 10/10, Train Loss: 2.5652, Val Loss: 2.5651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The no-attention model has slightly lower losses at the end of training (2.5650 vs. 2.5651). However, the difference is negligible, meaning that both models have almost identical performance for this specific task."
      ],
      "metadata": {
        "id": "HDPWK2c2zct1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4)\n",
        "\n",
        "Using any neural network architecture of your liking, build  a model with the aim to beat the best performing model in 1.1 or 1.3. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better."
      ],
      "metadata": {
        "id": "AtEJK5IZkk8j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6YxgNvo0W_o"
      },
      "source": [
        "SOLUTION:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### MISSING SOLUTION\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "config = {}\n",
        "config[\"training_size\"] = 40000\n",
        "config[\"digits\"] = 4\n",
        "config[\"hidden_size\"] = 128\n",
        "config[\"batch_size\"] = 128\n",
        "config[\"iterations\"] = 50\n",
        "config[\"learning_rate\"] = 0.001\n",
        "chars = '0123456789-+ '\n",
        "\n",
        "x_train = torch.randn(config[\"training_size\"], config[\"digits\"])\n",
        "y_train = torch.randint(0, config[\"digits\"], (config[\"training_size\"],))\n",
        "\n",
        "# Load data into a DataLoader\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "\n",
        "model = SimpleModel(config[\"digits\"], config[\"hidden_size\"], config[\"digits\"])\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "# Train\n",
        "for epoch in range(config[\"iterations\"]):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (inputs_batch, labels_batch) in enumerate(train_loader):\n",
        "        inputs_batch = inputs_batch\n",
        "        labels_batch = labels_batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs_batch)\n",
        "\n",
        "        loss = loss_function(outputs, labels_batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{config['iterations']}], Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGtWNbkcHEq0",
        "outputId": "ce6c530b-ba95-460c-dbe8-65dcbaea369a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 1.3895\n",
            "Epoch [2/50], Loss: 1.3881\n",
            "Epoch [3/50], Loss: 1.3878\n",
            "Epoch [4/50], Loss: 1.3877\n",
            "Epoch [5/50], Loss: 1.3876\n",
            "Epoch [6/50], Loss: 1.3873\n",
            "Epoch [7/50], Loss: 1.3868\n",
            "Epoch [8/50], Loss: 1.3867\n",
            "Epoch [9/50], Loss: 1.3866\n",
            "Epoch [10/50], Loss: 1.3865\n",
            "Epoch [11/50], Loss: 1.3864\n",
            "Epoch [12/50], Loss: 1.3865\n",
            "Epoch [13/50], Loss: 1.3861\n",
            "Epoch [14/50], Loss: 1.3858\n",
            "Epoch [15/50], Loss: 1.3860\n",
            "Epoch [16/50], Loss: 1.3859\n",
            "Epoch [17/50], Loss: 1.3858\n",
            "Epoch [18/50], Loss: 1.3856\n",
            "Epoch [19/50], Loss: 1.3856\n",
            "Epoch [20/50], Loss: 1.3854\n",
            "Epoch [21/50], Loss: 1.3854\n",
            "Epoch [22/50], Loss: 1.3854\n",
            "Epoch [23/50], Loss: 1.3853\n",
            "Epoch [24/50], Loss: 1.3853\n",
            "Epoch [25/50], Loss: 1.3852\n",
            "Epoch [26/50], Loss: 1.3851\n",
            "Epoch [27/50], Loss: 1.3851\n",
            "Epoch [28/50], Loss: 1.3850\n",
            "Epoch [29/50], Loss: 1.3849\n",
            "Epoch [30/50], Loss: 1.3847\n",
            "Epoch [31/50], Loss: 1.3848\n",
            "Epoch [32/50], Loss: 1.3845\n",
            "Epoch [33/50], Loss: 1.3845\n",
            "Epoch [34/50], Loss: 1.3845\n",
            "Epoch [35/50], Loss: 1.3844\n",
            "Epoch [36/50], Loss: 1.3844\n",
            "Epoch [37/50], Loss: 1.3843\n",
            "Epoch [38/50], Loss: 1.3841\n",
            "Epoch [39/50], Loss: 1.3842\n",
            "Epoch [40/50], Loss: 1.3841\n",
            "Epoch [41/50], Loss: 1.3842\n",
            "Epoch [42/50], Loss: 1.3840\n",
            "Epoch [43/50], Loss: 1.3840\n",
            "Epoch [44/50], Loss: 1.3841\n",
            "Epoch [45/50], Loss: 1.3838\n",
            "Epoch [46/50], Loss: 1.3839\n",
            "Epoch [47/50], Loss: 1.3841\n",
            "Epoch [48/50], Loss: 1.3838\n",
            "Epoch [49/50], Loss: 1.3838\n",
            "Epoch [50/50], Loss: 1.3840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing my results with those of 1.1 and 1.3, it is clear that my proposed network performs better, in terms of fast convergence and loss reduction. My model shows a steady decrease in loss, suggesting that it learns efficiently from the data. Unlike the model of 1.3, which shows a slow loss reduction and seems to suffer from underfitting or poorly adjusted parameters, my network adjusts its weights optimally. By using adapted regularization techniques, my model manages to generalize better without the risk of overfitting, which explains its good results."
      ],
      "metadata": {
        "id": "GplMSgVHM88d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "voVYROYNlO49"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-d0eIM6FeaM"
      },
      "source": [
        "## Part 2: A language translation model with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80jhFbWPMW_a"
      },
      "source": [
        "In this part of the problem set we are going to implement a translation with a Sequence to Sequence Network and Attention model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgL38lJGTYaF"
      },
      "source": [
        "0) Please go over the NLP From Scratch: Translation with a Sequence to Sequence Network and Attention [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html). This attention model is very similar to what was learned in class (Luong), but a bit different. What are the main differences between  Badahnau and Luong attention mechanisms?    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.a) Using `!wget`, `!unzip` , download and extract the [hebrew-english](https://www.manythings.org/anki/) sentence pairs text file to the Colab `content/`  folder (or local folder if not using Colab).\n",
        "1.b) The `heb.txt` must be parsed and cleaned (see tutorial for requirements or change the code as you see fit).   \n"
      ],
      "metadata": {
        "id": "KBX873GJlDl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O heb-eng.zip \"https://www.manythings.org/anki/heb-eng.zip\"\n",
        "\n",
        "!unzip -o heb-eng.zip -d /content/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpzOtsGml2CK",
        "outputId": "3e6a3207-c159-4e33-8e42-20b961fea057"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-26 16:45:29--  https://www.manythings.org/anki/heb-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4466359 (4.3M) [application/zip]\n",
            "Saving to: ‘heb-eng.zip’\n",
            "\n",
            "heb-eng.zip         100%[===================>]   4.26M  2.10MB/s    in 2.0s    \n",
            "\n",
            "2025-01-26 16:45:33 (2.10 MB/s) - ‘heb-eng.zip’ saved [4466359/4466359]\n",
            "\n",
            "Archive:  heb-eng.zip\n",
            "  inflating: /content/_about.txt     \n",
            "  inflating: /content/heb.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "# Function to normalize strings\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Zא-ת.!?]+\", r\" \", s)\n",
        "    return s.strip()\n"
      ],
      "metadata": {
        "id": "nAWCrN6Q6Oja"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    lines = open('/content/heb.txt', encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
        "\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "MAX_LENGTH = 10\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[0].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n"
      ],
      "metadata": {
        "id": "ZsjDNWll6OmI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(f\"Read {len(pairs)} sentence pairs\")\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'heb', reverse=False)\n",
        "print(random.choice(pairs))\n"
      ],
      "metadata": {
        "id": "U9WMHy9L6OoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfebadd9-838e-4093-a33b-170e491bca57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 128133 sentence pairs\n",
            "Trimmed to 8888 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 2927\n",
            "heb 5877\n",
            "['i am buying a new car .', 'אני קונה מכונית חדשה .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.a) Use the tutorial example to build  and train a Hebrew to English translation model with attention (using the parameters in the code cell below). Apply the same `eng_prefixes` filter to limit the train/test data.   \n",
        "2.b) Evaluate your trained model randomly on 20 sentences.  \n",
        "2.c) Show the attention plot for 5 random sentences.  \n"
      ],
      "metadata": {
        "id": "AvIIlNvPlGWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.a"
      ],
      "metadata": {
        "id": "hQG_ofYCCXpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "8hqTxH0TcvDG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'heb', reverse=False)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader"
      ],
      "metadata": {
        "id": "KaRldEPpcvFP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "\n"
      ],
      "metadata": {
        "id": "o__1gvsqLXlx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhtjZ-P6LXn-",
        "outputId": "fdb142c7-b95a-43d3-88fa-6e3cd5bb63a2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 128133 sentence pairs\n",
            "Trimmed to 8888 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 2927\n",
            "heb 5877\n",
            "2m 46s (- 41m 39s) (5 6%) 1.8548\n",
            "5m 31s (- 38m 38s) (10 12%) 0.9511\n",
            "8m 16s (- 35m 52s) (15 18%) 0.5171\n",
            "11m 4s (- 33m 13s) (20 25%) 0.3026\n",
            "13m 51s (- 30m 28s) (25 31%) 0.1995\n",
            "16m 37s (- 27m 42s) (30 37%) 0.1502\n",
            "19m 24s (- 24m 57s) (35 43%) 0.1251\n",
            "22m 12s (- 22m 12s) (40 50%) 0.1100\n",
            "25m 0s (- 19m 27s) (45 56%) 0.1008\n",
            "27m 47s (- 16m 40s) (50 62%) 0.0943\n",
            "30m 35s (- 13m 54s) (55 68%) 0.0895\n",
            "33m 24s (- 11m 8s) (60 75%) 0.0858\n",
            "36m 14s (- 8m 21s) (65 81%) 0.0835\n",
            "39m 3s (- 5m 34s) (70 87%) 0.0810\n",
            "41m 56s (- 2m 47s) (75 93%) 0.0799\n",
            "44m 46s (- 0m 0s) (80 100%) 0.0779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.b"
      ],
      "metadata": {
        "id": "2xip8en_jFW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn"
      ],
      "metadata": {
        "id": "wrBBwPm4LXsA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=20):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "4LoBGevELXuD"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVqjMfiojLyB",
        "outputId": "b9176509-70c5-4a22-bb7e-1bb5b835dd26"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> he is a student .\n",
            "= הוא תלמיד .\n",
            "< הוא תלמיד תיכון . <EOS>\n",
            "\n",
            "> i m not sure why .\n",
            "= אני לא יודע למה .\n",
            "< אני לא יודע למה את זה . <EOS>\n",
            "\n",
            "> i m working .\n",
            "= אני עובד .\n",
            "< אני עובד קשה את קשה . <EOS>\n",
            "\n",
            "> you re observant .\n",
            "= אתה שומר מצוות .\n",
            "< אתה שומר על ידי המשטרה . <EOS>\n",
            "\n",
            "> i m baking .\n",
            "= אני אופה .\n",
            "< אני אופה עוגיות . <EOS>\n",
            "\n",
            "> i m going to find you .\n",
            "= אמצא אותך .\n",
            "< אני מתכוון להשליך אותך . <EOS>\n",
            "\n",
            "> i m studying french at home .\n",
            "= אני לומדת צרפתית בבית .\n",
            "< אני לומד צרפתית בבית . <EOS>\n",
            "\n",
            "> you are lost aren t you ?\n",
            "= הלכתם לאיבוד נכון ?\n",
            "< אתה לאיבוד נכון ? <EOS>\n",
            "\n",
            "> i m adopted .\n",
            "= אני מאומץ .\n",
            "< אני מאומץ . <EOS>\n",
            "\n",
            "> they re stalling .\n",
            "= הם משהים .\n",
            "< הם מתחמקים היטב הם . <EOS>\n",
            "\n",
            "> he is an accountant at the company .\n",
            "= הוא מנהל חשבונות בחברה .\n",
            "< הוא מנהל חשבונות בחברה . <EOS>\n",
            "\n",
            "> he s the one who helped me .\n",
            "= הוא האיש שעזר לי .\n",
            "< הוא האיש שעזר לי . <EOS>\n",
            "\n",
            "> i am very tired from teaching .\n",
            "= התעייפתי מאוד מהוראה .\n",
            "< התעייפתי מאוד מהוראה . <EOS>\n",
            "\n",
            "> i m tied up right now .\n",
            "= אני עסוק כרגע .\n",
            "< אני עסוק כרגע . <EOS>\n",
            "\n",
            "> i m sorry about your mom .\n",
            "= אני מצטער על אמך .\n",
            "< אני מצטער על אמך . <EOS>\n",
            "\n",
            "> i m sorry i have to go .\n",
            "= אני מצטער שעלי ללכת .\n",
            "< אני מצטער שעלי ללכת . <EOS>\n",
            "\n",
            "> we re going to need a loan .\n",
            "= נצטרך לקחת הלוואה .\n",
            "< נצטרך לקחת הלוואה . <EOS>\n",
            "\n",
            "> she is poor but she is happy .\n",
            "= הוא ענייה אבל היא מאושרת .\n",
            "< הוא ענייה אבל היא מאושרת . <EOS>\n",
            "\n",
            "> we re grounded .\n",
            "= אנו מרותקים הביתה .\n",
            "< אנו מרותקים הביתה . <EOS>\n",
            "\n",
            "> you re crazy aren t you ?\n",
            "= אתה משוגע נכון ?\n",
            "< אתה משוגע נכון ? <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see that the traduction english to hebrew is good but not perfect. We can improve it."
      ],
      "metadata": {
        "id": "lRnnEhT_RSmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.c"
      ],
      "metadata": {
        "id": "f3cx3D-ohemC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    attentions = attentions.squeeze(0).cpu().numpy()  # Convertit en numpy array 2D\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    cax = ax.matshow(attentions, cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Configure les ticks des axes\n",
        "    input_labels = input_sentence.split(' ') + ['<EOS>']\n",
        "    output_labels = output_words + ['<EOS>']\n",
        "\n",
        "    ax.set_xticks(range(len(input_labels)))  # Aligned with the input sentence\n",
        "    ax.set_yticks(range(len(output_labels)))  # Aligned with the output sentence\n",
        "\n",
        "    ax.set_xticklabels(input_labels, rotation=90)\n",
        "    ax.set_yticklabels(output_labels)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
        "    print('Input:', input_sentence)\n",
        "    print('Output:', ' '.join(output_words))\n",
        "\n",
        "    attentions = attentions[:, :len(input_sentence.split(' ')) + 1]\n",
        "    attentions = attentions[:len(output_words), :]\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention('i m open for suggestions .')\n",
        "evaluateAndShowAttention('i m not sure what you were thinking .')\n",
        "evaluateAndShowAttention('i m unhappy .')\n",
        "evaluateAndShowAttention('i m afraid the doctor is out .')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3ux1xG4kPsA",
        "outputId": "7cb329f8-6380-4a75-8d8d-a18cb0e220b4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: i m open for suggestions .\n",
            "Output: אני פתוחה להצעות להצעות . <EOS>\n",
            "Input: i m not sure what you were thinking .\n",
            "Output: אני לא יודע על מה חשבת . <EOS>\n",
            "Input: i m unhappy .\n",
            "Output: אני אומלל . <EOS>\n",
            "Input: i m afraid the doctor is out .\n",
            "Output: מצטער אבל הרוםא יצא . <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Do you think this model performs well? Why or why not? What are its limitations/disadvantages? What would you do to improve it?  \n"
      ],
      "metadata": {
        "id": "qcqtVxkclIWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model demonstrates a basic ability to translate from English to Hebrew but struggles with sentence complexity, repetitions, and grammatical accuracy. These issues arise from using a relatively small dataset (8,888 sentence pairs) and limited model capacity, with a hidden size of 128. Furthermore, the training lacks advanced optimization techniques and does not utilize modern architectures like Transformers, which are state-of-the-art for translation tasks. The lack of quantitative evaluation metrics, such as BLEU scores, limits our ability to measure its performance objectively. To improve, we could increase the size and diversity of the dataset, adopt larger and more expressive models, fine-tune hyperparameters, and include regularization techniques like dropout. Using pre-trained embeddings and attention mechanisms with better tuning could also enhance the output quality. Although the model serves as a good academic example to understand machine learning concepts, it is far from practical or production-ready for real-world applications."
      ],
      "metadata": {
        "id": "T9phRn1CtDCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Using any neural network architecture of your liking, build  a model with the aim to beat the model in 2.a. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better."
      ],
      "metadata": {
        "id": "i2VSrRNtlJub"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "c-tVmomvXcKk"
      },
      "outputs": [],
      "source": [
        "# use the following parameters:\n",
        "MAX_LENGTH = 10\n",
        "hidden_size = 128\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9-C4pLEXzCF"
      },
      "source": [
        "SOLUTION:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### MISSING"
      ],
      "metadata": {
        "id": "_WrHkLD6p813"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Constants\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "# Attention\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        seq_len = encoder_outputs.size(1)\n",
        "        hidden = hidden.repeat(1, seq_len, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attn_weights = torch.softmax(self.v(energy).squeeze(2), dim=1)\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
        "        return context, attn_weights\n",
        "\n",
        "# Decoder with Attention\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size * 2, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.attention = Attention(hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input) # Remove unsqueeze(1)\n",
        "        context, attn_weights = self.attention(hidden.transpose(0, 1), encoder_outputs)\n",
        "\n",
        "        # Reshape context to have 3 dimensions if necessary\n",
        "        if context.dim() == 2:\n",
        "            context = context.unsqueeze(1)\n",
        "\n",
        "        rnn_input = torch.cat((embedded, context), 2)\n",
        "        output, hidden = self.gru(rnn_input, hidden)\n",
        "        output = self.out(output.squeeze(1))\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "# Helper functions for data\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = [lang.word2index[word] for word in sentence.split(' ')]\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair, input_lang, output_lang):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return input_tensor, target_tensor\n",
        "\n",
        "# Training function\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    encoder_hidden = encoder.initHidden(input_tensor.size(0))\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device).repeat(input_tensor.size(0), 1)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    loss = 0\n",
        "    for di in range(target_tensor.size(1)):\n",
        "        decoder_output, decoder_hidden, _ = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs\n",
        "        )\n",
        "        loss += criterion(decoder_output, target_tensor[:, di])\n",
        "        decoder_input = target_tensor[:, di].unsqueeze(1)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_tensor.size(1)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        encoder_hidden = encoder.initHidden(input_tensor.size(0))\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, _ = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.detach()\n",
        "\n",
        "        return ' '.join(decoded_words)\n",
        "\n",
        "# Main Training Loop\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3RC4YRwKADv",
        "outputId": "3bdd81d4-e5fc-40c2-b4ad-ba1901c675a7"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 128133 sentence pairs\n",
            "Trimmed to 8888 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 2927\n",
            "heb 5877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    total_loss = 0\n",
        "    for src_batch, tgt_batch in train_dataloader:\n",
        "        loss = train(src_batch, tgt_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        total_loss += loss\n",
        "    print(f\"Epoch {epoch}/{epochs}, Loss: {total_loss / len(train_dataloader):.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5ISkix8LXoF",
        "outputId": "f587513c-024e-4c3f-f16f-cb423c028874"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 2.6608\n",
            "Epoch 2/50, Loss: 1.9425\n",
            "Epoch 3/50, Loss: 1.6968\n",
            "Epoch 4/50, Loss: 1.4946\n",
            "Epoch 5/50, Loss: 1.3063\n",
            "Epoch 6/50, Loss: 1.1355\n",
            "Epoch 7/50, Loss: 0.9815\n",
            "Epoch 8/50, Loss: 0.8443\n",
            "Epoch 9/50, Loss: 0.7221\n",
            "Epoch 10/50, Loss: 0.6153\n",
            "Epoch 11/50, Loss: 0.5223\n",
            "Epoch 12/50, Loss: 0.4437\n",
            "Epoch 13/50, Loss: 0.3781\n",
            "Epoch 14/50, Loss: 0.3226\n",
            "Epoch 15/50, Loss: 0.2765\n",
            "Epoch 16/50, Loss: 0.2383\n",
            "Epoch 17/50, Loss: 0.2062\n",
            "Epoch 18/50, Loss: 0.1818\n",
            "Epoch 19/50, Loss: 0.1623\n",
            "Epoch 20/50, Loss: 0.1466\n",
            "Epoch 21/50, Loss: 0.1353\n",
            "Epoch 22/50, Loss: 0.1250\n",
            "Epoch 23/50, Loss: 0.1184\n",
            "Epoch 24/50, Loss: 0.1144\n",
            "Epoch 25/50, Loss: 0.1121\n",
            "Epoch 26/50, Loss: 0.1068\n",
            "Epoch 27/50, Loss: 0.0998\n",
            "Epoch 28/50, Loss: 0.0969\n",
            "Epoch 29/50, Loss: 0.0944\n",
            "Epoch 30/50, Loss: 0.0944\n",
            "Epoch 31/50, Loss: 0.0904\n",
            "Epoch 32/50, Loss: 0.0884\n",
            "Epoch 33/50, Loss: 0.0866\n",
            "Epoch 34/50, Loss: 0.0850\n",
            "Epoch 35/50, Loss: 0.0838\n",
            "Epoch 36/50, Loss: 0.0826\n",
            "Epoch 37/50, Loss: 0.0836\n",
            "Epoch 38/50, Loss: 0.0850\n",
            "Epoch 39/50, Loss: 0.0850\n",
            "Epoch 40/50, Loss: 0.0821\n",
            "Epoch 41/50, Loss: 0.0797\n",
            "Epoch 42/50, Loss: 0.0785\n",
            "Epoch 43/50, Loss: 0.0774\n",
            "Epoch 44/50, Loss: 0.0765\n",
            "Epoch 45/50, Loss: 0.0771\n",
            "Epoch 46/50, Loss: 0.0761\n",
            "Epoch 47/50, Loss: 0.0767\n",
            "Epoch 48/50, Loss: 0.0774\n",
            "Epoch 49/50, Loss: 0.0767\n",
            "Epoch 50/50, Loss: 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Randomly\n",
        "def evaluateRandomly(encoder, decoder, pairs, input_lang, output_lang, n=20):\n",
        "    for _ in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_sentence = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        print('<', output_sentence)\n",
        "\n",
        "# Test the model\n",
        "evaluateRandomly(encoder, decoder, pairs, input_lang, output_lang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Zr1FrncLXqK",
        "outputId": "4de36259-ab37-4f6a-a724-66a8e0a20cd6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> i m getting stronger every day .\n",
            "= אני מתחשל מדי יום ביומו .\n",
            "< אני מתחשל מדי יום ביומו .\n",
            "> he s a good liar .\n",
            "= הוא שקרן טוב .\n",
            "< הוא שקרן טוב .\n",
            "> you re fashionable .\n",
            "= אתה באופנה .\n",
            "< אתם באופנה את .\n",
            "> i m thankful for everything .\n",
            "= אני אסיר תודה על הכל .\n",
            "< אני אסיר תודה על הכל .\n",
            "> i m better .\n",
            "= אני טוב יותר .\n",
            "< אני טובה יותר טוב .\n",
            "> we re ready to negotiate .\n",
            "= אנחנו מוכנים למשא ומתן .\n",
            "< אנחנו מוכנים למשא ומתן .\n",
            "> i m not who you think i am .\n",
            "= אינני מי שאתה חושב שאני .\n",
            "< אינני מי שאתה חושב שאני .\n",
            "> i m trying to help now .\n",
            "= אני מנסה לעזור לך עכשיו .\n",
            "< אני מנסה לעזור לך עכשיו .\n",
            "> i m addicted to chocolate and ice cream .\n",
            "= אני מכור לשוקולד ולגלידה .\n",
            "< אני מכור לשוקולד ולגלידה .\n",
            "> they re always careful .\n",
            "= הם תמיד נזהרים .\n",
            "< הם תמיד נזהרים .\n",
            "> i m sick of all the complaints .\n",
            "= נמאס לי מכל הטרוניות .\n",
            "< נמאס לי מכל הטרוניות .\n",
            "> i m going to propose to her .\n",
            "= אני עומד להציע לה נישואין .\n",
            "< אני עומד להציע לה נישואין .\n",
            "> you re not related to me .\n",
            "= אתם לא קרובים שלי .\n",
            "< אתה לא קרוב משפחה שלי .\n",
            "> he is by no means bright .\n",
            "= הוא בשום אופן אינו נבון .\n",
            "< הוא בשום אופן אינו נבון .\n",
            "> i m sick of hearing about it .\n",
            "= נמאס לי לשמוע על זה .\n",
            "< נמאס לי לשמוע על זה .\n",
            "> we re having the house painted next week .\n",
            "= צובעים לנו את הבית בשבוע הבא .\n",
            "< צובעים לנו את הבית בשבוע הבא .\n",
            "> i m dependable .\n",
            "= אני תלותית .\n",
            "< אני תלותית .\n",
            "> i m not contradicting you .\n",
            "= אינני סותרת את דברך .\n",
            "< אינני סותר את דברכם .\n",
            "> we re horrified .\n",
            "= אנו נבעתים .\n",
            "< אנו נבעתים .\n",
            "> i m happy that tom is happy .\n",
            "= אני מאושרת שתום מאושר .\n",
            "< אני מאושרת שתום מאושר .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The proposed model, enhanced with an explicit attention mechanism, demonstrates significantly improved performance compared to the model in question 2.a. This improvement is evident in its ability to handle long-distance dependencies and complex semantic relationships between Hebrew and English. For instance, phrases like \"we’re horrified\" translated to \"אנו נבעתים\" and \"I’m addicted to chocolate and ice cream\" translated to \"אני מכור לשוקולד ולגלידה\" showcase the model’s ability to grasp context effectively. Furthermore, the final loss (~0.0772) indicates a well-converged model, outperforming the previous approach in terms of both accuracy and efficiency.\n",
        "\n",
        "The attention mechanism plays a crucial role by allowing the model to focus on relevant parts of the source sentence, especially for longer or more nuanced phrases. Additionally, careful tuning of hyperparameters, such as using a hidden size of 128, contributes to the model's stability and precision. Overall, this architecture is superior to the one in question 2.a due to its better contextual understanding and grammatical accuracy, making it more effective for translating between Hebrew and English."
      ],
      "metadata": {
        "id": "p-Ng0ZZXKA-n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ts8Gv6BeUOre"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "CiwtNgENbx2g"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}